{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3404a0c-0a37-4253-85fb-9a052b934d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age'], dtype='object')\n",
      "Logistic Regression Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Logistic Regression saved as 'logistic_regression_model.pkl'\n",
      "\n",
      "Support Vector Machine Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Support Vector Machine saved as 'support_vector_machine_model.pkl'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0000\n",
      "[[  2   0]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00       152\n",
      "\n",
      "    accuracy                           1.00       154\n",
      "   macro avg       1.00      1.00      1.00       154\n",
      "weighted avg       1.00      1.00      1.00       154\n",
      "\n",
      "Random Forest saved as 'random_forest_model.pkl'\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Hypertension\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load the scaler\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Scale the new data\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m new_data_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(new_data)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Make predictions on the new data using the Logistic Regression model\u001b[39;00m\n\u001b[0;32m     97\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load the model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m   1063\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1064\u001b[0m     X,\n\u001b[0;32m   1065\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1066\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1067\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1068\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1069\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1070\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1071\u001b[0m )\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2919\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[0;32m   2836\u001b[0m     _estimator,\n\u001b[0;32m   2837\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m   2844\u001b[0m ):\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \n\u001b[0;32m   2847\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2919\u001b[0m     _check_feature_names(_estimator, X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m   2920\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[0;32m   2921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2777\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m   2775\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2777\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Hypertension\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib  # Import joblib for saving the model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_diabetes_103.csv')\n",
    "\n",
    "# Print the column names to check what is available\n",
    "print(\"Columns in the dataset:\", data.columns)\n",
    "\n",
    "# Define the prediabetes criteria\n",
    "data['Prediabetes'] = ((data['Glucose'] >= 90) & (data['Glucose'] <= 125)) | \\\n",
    "                       (data['Insulin'] > 25) | \\\n",
    "                       (data['BloodPressure'] > 130)\n",
    "\n",
    "# Convert boolean to integer (1 for prediabetes, 0 for not)\n",
    "data['Prediabetes'] = data['Prediabetes'].astype(int)\n",
    "\n",
    "# Create the Hypertension feature based on the new risk conditions\n",
    "data['Hypertension'] = 0  # Initialize the column\n",
    "\n",
    "# Condition 1: Risk of 50% for hypertension if Insulin > 25 and Glucose > 100\n",
    "risk_condition_1 = (data['Insulin'] > 25) & (data['Glucose'] > 100)\n",
    "\n",
    "# Assign Hypertension based on the first risk condition\n",
    "data.loc[risk_condition_1 & (data['BloodPressure'] > 130), 'Hypertension'] = 1\n",
    "\n",
    "# Condition 2: Risk of 80% for hypertension if Glucose > 125\n",
    "risk_condition_2 = data['Glucose'] > 125\n",
    "\n",
    "# Assign Hypertension based on the second risk condition\n",
    "data.loc[risk_condition_2, 'Hypertension'] = 1\n",
    "\n",
    "# Fill Hypertension for those not meeting the conditions\n",
    "data['Hypertension'] = data['Hypertension'].fillna(0).astype(int)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(['Prediabetes'], axis=1)  # Exclude the target variable\n",
    "y = data['Prediabetes']  # New target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train models and evaluate their performance\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test_scaled)  # Make predictions\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save the model to a file\n",
    "    joblib.dump(model, f'{model_name.replace(\" \", \"_\").lower()}_model.pkl')\n",
    "    print(f\"{model_name} saved as '{model_name.replace(' ', '_').lower()}_model.pkl'\\n\")\n",
    "\n",
    "# Example of making predictions on new data\n",
    "# Define new data (example values)\n",
    "new_data = pd.DataFrame({\n",
    "    'Glucose': [130],\n",
    "    'BloodPressure': [70],\n",
    "    'SkinThickness': [35],\n",
    "    'Insulin': [0],\n",
    "    'BMI': [33.6],\n",
    "    'Age': [50]\n",
    "})\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load('scaler.pkl')  # Load the scaler\n",
    "\n",
    "# Scale the new data\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions on the new data using the Logistic Regression model\n",
    "model = joblib.load('logistic_regression_model.pkl')  # Load the model\n",
    "predictions = model.predict(new_data_scaled)\n",
    "\n",
    "# Output the predictions\n",
    "print(\"Predictions for new data:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee15a21-9c64-4b6c-b6c2-c5d10d1b777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age'], dtype='object')\n",
      "Logistic Regression Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Logistic Regression saved as 'logistic_regression_model.pkl'\n",
      "\n",
      "Support Vector Machine Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Support Vector Machine saved as 'support_vector_machine_model.pkl'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0000\n",
      "[[  2   0]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00       152\n",
      "\n",
      "    accuracy                           1.00       154\n",
      "   macro avg       1.00      1.00      1.00       154\n",
      "weighted avg       1.00      1.00      1.00       154\n",
      "\n",
      "Random Forest saved as 'random_forest_model.pkl'\n",
      "\n",
      "Predictions for new data: [1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib  # Import joblib for saving the model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_diabetes_103.csv')\n",
    "\n",
    "# Print the column names to check what is available\n",
    "print(\"Columns in the dataset:\", data.columns)\n",
    "\n",
    "# Define the prediabetes criteria\n",
    "data['Prediabetes'] = ((data['Glucose'] >= 90) & (data['Glucose'] <= 125)) | \\\n",
    "                       (data['Insulin'] > 25) | \\\n",
    "                       (data['BloodPressure'] > 130)\n",
    "\n",
    "# Convert boolean to integer (1 for prediabetes, 0 for not)\n",
    "data['Prediabetes'] = data['Prediabetes'].astype(int)\n",
    "\n",
    "# Create the Hypertension feature based on the new risk conditions\n",
    "data['Hypertension'] = 0  # Initialize the column\n",
    "\n",
    "# Condition 1: Risk of 50% for hypertension if Insulin > 25 and Glucose > 100\n",
    "risk_condition_1 = (data['Insulin'] > 25) & (data['Glucose'] > 100)\n",
    "\n",
    "# Assign Hypertension based on the first risk condition\n",
    "data.loc[risk_condition_1 & (data['BloodPressure'] > 130), 'Hypertension'] = 1\n",
    "\n",
    "# Condition 2: Risk of 80% for hypertension if Glucose > 125\n",
    "risk_condition_2 = data['Glucose'] > 125\n",
    "\n",
    "# Assign Hypertension based on the second risk condition\n",
    "data.loc[risk_condition_2, 'Hypertension'] = 1\n",
    "\n",
    "# Fill Hypertension for those not meeting the conditions\n",
    "data['Hypertension'] = data['Hypertension'].fillna(0).astype(int)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(['Prediabetes'], axis=1)  # Exclude the target variable\n",
    "y = data['Prediabetes']  # New target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train models and evaluate their performance\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test_scaled)  # Make predictions\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save the model to a file\n",
    "    joblib.dump(model, f'{model_name.replace(\" \", \"_\").lower()}_model.pkl')\n",
    "    print(f\"{model_name} saved as '{model_name.replace(' ', '_').lower()}_model.pkl'\\n\")\n",
    "\n",
    "# Example of making predictions on new data\n",
    "# Define new data (example values)\n",
    "new_data = pd.DataFrame({\n",
    "    'Glucose': [130],\n",
    "    'BloodPressure': [70],\n",
    "    'SkinThickness': [35],\n",
    "    'Insulin': [0],\n",
    "    'BMI': [33.6],\n",
    "    'Age': [50],\n",
    "    'Hypertension': [0]  # Include the missing feature 'Hypertension'\n",
    "})\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load('scaler.pkl')  # Load the scaler\n",
    "\n",
    "# Scale the new data\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions on the new data using the Logistic Regression model\n",
    "model = joblib.load('logistic_regression_model.pkl')  # Load the model\n",
    "predictions = model.predict(new_data_scaled)\n",
    "\n",
    "# Output the predictions\n",
    "print(\"Predictions for new data:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae0a1a0-e8d1-4d27-84c6-6dd1cdd2c6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age'], dtype='object')\n",
      "Logistic Regression Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Logistic Regression saved as 'logistic_regression_model.pkl'\n",
      "\n",
      "Support Vector Machine Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Support Vector Machine saved as 'support_vector_machine_model.pkl'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0000\n",
      "[[  2   0]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00       152\n",
      "\n",
      "    accuracy                           1.00       154\n",
      "   macro avg       1.00      1.00      1.00       154\n",
      "weighted avg       1.00      1.00      1.00       154\n",
      "\n",
      "Random Forest saved as 'random_forest_model.pkl'\n",
      "\n",
      "Predictions for Logistic Regression on new data: [1]\n",
      "Predictions for Support Vector Machine on new data: [1]\n",
      "Predictions for Random Forest on new data: [1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib  # Import joblib for saving the model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_diabetes_103.csv')\n",
    "\n",
    "# Print the column names to check what is available\n",
    "print(\"Columns in the dataset:\", data.columns)\n",
    "\n",
    "# Define the prediabetes criteria\n",
    "data['Prediabetes'] = ((data['Glucose'] >= 90) & (data['Glucose'] <= 125)) | \\\n",
    "                       (data['Insulin'] > 25) | \\\n",
    "                       (data['BloodPressure'] > 130)\n",
    "\n",
    "# Convert boolean to integer (1 for prediabetes, 0 for not)\n",
    "data['Prediabetes'] = data['Prediabetes'].astype(int)\n",
    "\n",
    "# Create the Hypertension feature based on the new risk conditions\n",
    "data['Hypertension'] = 0  # Initialize the column\n",
    "\n",
    "# Condition 1: Risk of 50% for hypertension if Insulin > 25 and Glucose > 100\n",
    "risk_condition_1 = (data['Insulin'] > 25) & (data['Glucose'] > 100)\n",
    "\n",
    "# Assign Hypertension based on the first risk condition\n",
    "data.loc[risk_condition_1 & (data['BloodPressure'] > 130), 'Hypertension'] = 1\n",
    "\n",
    "# Condition 2: Risk of 80% for hypertension if Glucose > 125\n",
    "risk_condition_2 = data['Glucose'] > 125\n",
    "\n",
    "# Assign Hypertension based on the second risk condition\n",
    "data.loc[risk_condition_2, 'Hypertension'] = 1\n",
    "\n",
    "# Fill Hypertension for those not meeting the conditions\n",
    "data['Hypertension'] = data['Hypertension'].fillna(0).astype(int)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(['Prediabetes'], axis=1)  # Exclude the target variable\n",
    "y = data['Prediabetes']  # New target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train models and evaluate their performance\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test_scaled)  # Make predictions\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save the model to a file\n",
    "    joblib.dump(model, f'{model_name.replace(\" \", \"_\").lower()}_model.pkl')\n",
    "    print(f\"{model_name} saved as '{model_name.replace(' ', '_').lower()}_model.pkl'\\n\")\n",
    "\n",
    "# Example of making predictions on new data\n",
    "# Define new data (example values)\n",
    "new_data = pd.DataFrame({\n",
    "    'Glucose': [130],\n",
    "    'BloodPressure': [70],\n",
    "    'SkinThickness': [35],\n",
    "    'Insulin': [0],\n",
    "    'BMI': [33.6],\n",
    "    'Age': [50],\n",
    "    'Hypertension': [0]  # Include the missing feature 'Hypertension'\n",
    "})\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load('scaler.pkl')  # Load the scaler\n",
    "\n",
    "# Scale the new data\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the new data using all models\n",
    "for model_name in models.keys():\n",
    "    model = joblib.load(f'{model_name.replace(\" \", \"_\").lower()}_model.pkl')  # Load the model\n",
    "    predictions = model.predict(new_data_scaled)  # Make predictions\n",
    "    print(f\"Predictions for {model_name} on new data:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d31b17ae-9ab7-4146-88a8-aeac9df99933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age'], dtype='object')\n",
      "Logistic Regression Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Logistic Regression Cross-Validation Accuracy: 0.9919 ± 0.0000\n",
      "Logistic Regression saved as 'logistic_regression_model.pkl'\n",
      "\n",
      "Support Vector Machine Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Support Vector Machine Cross-Validation Accuracy: 0.9919 ± 0.0000\n",
      "Support Vector Machine saved as 'support_vector_machine_model.pkl'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9935\n",
      "[[  1   1]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.99      1.00      1.00       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       1.00      0.75      0.83       154\n",
      "weighted avg       0.99      0.99      0.99       154\n",
      "\n",
      "Random Forest Cross-Validation Accuracy: 0.9919 ± 0.0000\n",
      "Random Forest saved as 'random_forest_model.pkl'\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- BMI\n- Hypertension\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load the scaler\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Scale the new data\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m new_data_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(new_data)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Make predictions on the new data using all models\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m   1063\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1064\u001b[0m     X,\n\u001b[0;32m   1065\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1066\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1067\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1068\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1069\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1070\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1071\u001b[0m )\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2919\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[0;32m   2836\u001b[0m     _estimator,\n\u001b[0;32m   2837\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m   2844\u001b[0m ):\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \n\u001b[0;32m   2847\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2919\u001b[0m     _check_feature_names(_estimator, X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m   2920\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[0;32m   2921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2777\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m   2775\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2777\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- BMI\n- Hypertension\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df131402-84ab-44ce-903f-07c2fcfcc272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8997b79-d711-4fb2-afb0-439e1fbe7ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age'], dtype='object')\n",
      "Logistic Regression Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Logistic Regression saved as 'logistic_regression_model.pkl'\n",
      "\n",
      "Support Vector Machine Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Support Vector Machine saved as 'support_vector_machine_model.pkl'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0000\n",
      "[[  2   0]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00       152\n",
      "\n",
      "    accuracy                           1.00       154\n",
      "   macro avg       1.00      1.00      1.00       154\n",
      "weighted avg       1.00      1.00      1.00       154\n",
      "\n",
      "Random Forest saved as 'random_forest_model.pkl'\n",
      "\n",
      "Please enter your details:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  aditya\n",
      "Enter Insulin level:  20\n",
      "Enter Glucose level:  100\n",
      "Enter Blood Pressure level:  110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aditya, Prediction from Logistic Regression: Prediabetes\n",
      "aditya, Prediction from Support Vector Machine: Prediabetes\n",
      "aditya, Prediction from Random Forest: Prediabetes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib  # Import joblib for saving the model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_diabetes_103.csv')\n",
    "\n",
    "# Print the column names to check what is available\n",
    "print(\"Columns in the dataset:\", data.columns)\n",
    "\n",
    "# Define the prediabetes criteria\n",
    "data['Prediabetes'] = ((data['Glucose'] >= 90) & (data['Glucose'] <= 125)) | \\\n",
    "                       (data['Insulin'] > 25) | \\\n",
    "                       (data['BloodPressure'] > 130)\n",
    "\n",
    "# Convert boolean to integer (1 for prediabetes, 0 for not)\n",
    "data['Prediabetes'] = data['Prediabetes'].astype(int)\n",
    "\n",
    "# Create the Hypertension feature based on the new risk conditions\n",
    "data['Hypertension'] = 0  # Initialize the column\n",
    "\n",
    "# Condition 1: Risk of 50% for hypertension if Insulin > 25 and Glucose > 100\n",
    "risk_condition_1 = (data['Insulin'] > 25) & (data['Glucose'] > 100)\n",
    "\n",
    "# Assign Hypertension based on the first risk condition\n",
    "data.loc[risk_condition_1 & (data['BloodPressure'] > 130), 'Hypertension'] = 1\n",
    "\n",
    "# Condition 2: Risk of 80% for hypertension if Glucose > 125\n",
    "risk_condition_2 = data['Glucose'] > 125\n",
    "\n",
    "# Assign Hypertension based on the second risk condition\n",
    "data.loc[risk_condition_2, 'Hypertension'] = 1\n",
    "\n",
    "# Fill Hypertension for those not meeting the conditions\n",
    "data['Hypertension'] = data['Hypertension'].fillna(0).astype(int)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['Glucose', 'Insulin', 'BloodPressure']]  # Only include relevant features\n",
    "y = data['Prediabetes']  # New target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train models and evaluate their performance\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test_scaled)  # Make predictions\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save the model to a file\n",
    "    joblib.dump(model, f'{model_name.replace(\" \", \"_\").lower()}_model.pkl')\n",
    "    print(f\"{model_name} saved as '{model_name.replace(' ', '_').lower()}_model.pkl'\\n\")\n",
    "\n",
    "# Function to get user input\n",
    "def get_user_input():\n",
    "    name = input(\"Enter your name: \")\n",
    "    insulin = float(input(\"Enter Insulin level: \"))\n",
    "    glucose = float(input(\"Enter Glucose level: \"))\n",
    "    blood_pressure = float(input(\"Enter Blood Pressure level: \"))\n",
    "    \n",
    "    # Create a DataFrame for the input\n",
    "    new_data = pd.DataFrame({\n",
    "        'Glucose': [glucose],\n",
    "        'Insulin': [insulin],\n",
    "        'BloodPressure': [blood_pressure]\n",
    "    })\n",
    "    \n",
    "    return name, new_data\n",
    "\n",
    "# Function to make predictions\n",
    "def make_predictions(new_data):\n",
    "    # Load the scaler\n",
    "    scaler = joblib.load('scaler.pkl')  # Load the scaler\n",
    "\n",
    "    # Scale the new data\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "    # Make predictions on the new data using all models\n",
    "    predictions = {}\n",
    "    for model_name in models.keys():\n",
    "        model = joblib.load(f'{model_name.replace(\" \", \"_\").lower()}_model.pkl')  # Load the model\n",
    "        pred = model.predict(new_data_scaled)  # Make predictions\n",
    "        predictions[model_name] = pred[0]  # Store the prediction\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Main function to run the input and prediction process\n",
    "def main():\n",
    "    print(\"Please enter your details:\")\n",
    "    name, new_data = get_user_input()  # Get user input\n",
    "    predictions = make_predictions(new_data)  # Make predictions\n",
    "\n",
    "    # Output the predictions\n",
    "    for model_name, prediction in predictions.items():\n",
    "        if prediction == 1:\n",
    "            print(f\"{name}, Prediction from {model_name}: Prediabetes\")\n",
    "        else:\n",
    "            print(f\"{name}, Prediction from {model_name}: No Prediabetes\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b6c5843-700c-45b8-863c-fe30931ca0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age'], dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (614, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Train models and evaluate their performance\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 66\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)  \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)  \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1220\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1222\u001b[0m X, y \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m   1223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1224\u001b[0m     X,\n\u001b[0;32m   1225\u001b[0m     y,\n\u001b[0;32m   1226\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1227\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_dtype,\n\u001b[0;32m   1228\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1229\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1230\u001b[0m )\n\u001b[0;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1387\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m   1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1371\u001b[0m     X,\n\u001b[0;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1385\u001b[0m )\n\u001b[1;32m-> 1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1407\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m-> 1408\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1409\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1410\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1485\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn, device)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1473\u001b[0m             (\n\u001b[0;32m   1474\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1479\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1480\u001b[0m         )\n\u001b[0;32m   1481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(\n\u001b[0;32m   1482\u001b[0m         xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m   1483\u001b[0m     )\n\u001b[1;32m-> 1485\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1486\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1487\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (614, 2) instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib  # Import joblib for saving the model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_diabetes_103.csv')\n",
    "\n",
    "# Print the column names to check what is available\n",
    "print(\"Columns in the dataset:\", data.columns)\n",
    "\n",
    "# Define the prediabetes criteria\n",
    "data['Prediabetes'] = ((data['Glucose'] >= 90) & (data['Glucose'] <= 125)) | \\\n",
    "                       (data['Insulin'] > 25) | \\\n",
    "                       (data['BloodPressure'] > 130)\n",
    "\n",
    "# Convert boolean to integer (1 for prediabetes, 0 for not)\n",
    "data['Prediabetes'] = data['Prediabetes'].astype(int)\n",
    "\n",
    "# Create the Hypertension feature based on the new risk conditions\n",
    "data['Hypertension'] = 0  # Initialize the column\n",
    "\n",
    "# Condition 1: Risk of 50% for hypertension if Insulin > 25 and Glucose > 100\n",
    "risk_condition_1 = (data['Insulin'] > 25) & (data['Glucose'] > 100)\n",
    "\n",
    "# Assign Hypertension based on the first risk condition\n",
    "data.loc[risk_condition_1 & (data['BloodPressure'] > 130), 'Hypertension'] = 1\n",
    "\n",
    "# Condition 2: Risk of 80% for hypertension if Glucose > 125\n",
    "risk_condition_2 = data['Glucose'] > 125\n",
    "\n",
    "# Assign Hypertension based on the second risk condition\n",
    "data.loc[risk_condition_2, 'Hypertension'] = 1\n",
    "\n",
    "# Fill Hypertension for those not meeting the conditions\n",
    "data['Hypertension'] = data['Hypertension'].fillna(0).astype(int)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['Glucose', 'Insulin', 'BloodPressure', 'Age']]  # Include Age as a feature\n",
    "y = data[['Prediabetes', 'Hypertension']]  # New target variables\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train models and evaluate their performance\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test_scaled)  # Make predictions\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save the model to a file\n",
    "    joblib.dump(model, f'{model_name.replace(\" \", \"_\").lower()}_model.pkl')\n",
    "    print(f\"{model_name} saved as '{model_name.replace(' ', '_').lower()}_model.pkl'\\n\")\n",
    "\n",
    "# Function to get user input\n",
    "def get_user_input():\n",
    "    name = input(\"Enter your name: \")\n",
    "    insulin = float(input(\"Enter Insulin level: \"))\n",
    "    glucose = float(input(\"Enter Glucose level: \"))\n",
    "    blood_pressure = float(input(\"Enter Blood Pressure level: \"))\n",
    "    age = int(input(\"Enter Age: \"))\n",
    "    \n",
    "    # Create a DataFrame for the input\n",
    "    new_data = pd.DataFrame({\n",
    "        'Glucose': [glucose],\n",
    "        'Insulin': [insulin],\n",
    "        'BloodPressure': [blood_pressure],\n",
    "        'Age': [age]\n",
    "    })\n",
    "    \n",
    "    return name, new_data\n",
    "\n",
    "# Function to make predictions\n",
    "def make_predictions(new_data):\n",
    "    # Load the scaler\n",
    "    scaler = joblib.load('scaler.pkl')  # Load the scaler\n",
    "\n",
    "    # Scale the new data\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "    # Make predictions on the new data using all models\n",
    "    predictions = {}\n",
    "    for model_name, model in models.items():\n",
    "        model = joblib.load(f'{model_name.replace(\" \", \"_\").lower()}_model.pkl')  # Load the model\n",
    "        pred = model.predict(new_data_scaled)  # Make predictions\n",
    "        predictions[model_name] = pred[0]  # Store the prediction\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Main function to run the input and prediction process\n",
    "def main():\n",
    "    print(\"Please enter your details:\")\n",
    "    name, new_data = get_user_input()  # Get user input\n",
    "    predictions = make_predictions(new_data)  # Make predictions\n",
    "\n",
    "    # Output the predictions\n",
    "    for model_name, prediction in predictions.items():\n",
    "        if prediction[0] == 1:\n",
    "            print(f\"{name}, Prediction from {model_name}: Prediabetes\")\n",
    "        else:\n",
    "            print(f\"{name}, Prediction from {model_name}: No Prediabetes\")\n",
    "        \n",
    "        if prediction[1] == 1:\n",
    "            print(f\"{name}, Prediction from {model_name}: Hypertension\")\n",
    "        else:\n",
    "            print(f\"{name}, Prediction from {model_name}: No Hypertension\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a00c2cfe-25d1-440c-b096-5b17f209e4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age'], dtype='object')\n",
      "Logistic Regression (Prediabetes) Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Logistic Regression saved as 'logistic_regression_prediabetes_model.pkl'\n",
      "\n",
      "Support Vector Machine (Prediabetes) Accuracy: 0.9870\n",
      "[[  0   2]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.99      1.00      0.99       152\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.49      0.50      0.50       154\n",
      "weighted avg       0.97      0.99      0.98       154\n",
      "\n",
      "Support Vector Machine saved as 'support_vector_machine_prediabetes_model.pkl'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Prediabetes) Accuracy: 1.0000\n",
      "[[  2   0]\n",
      " [  0 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00       152\n",
      "\n",
      "    accuracy                           1.00       154\n",
      "   macro avg       1.00      1.00      1.00       154\n",
      "weighted avg       1.00      1.00      1.00       154\n",
      "\n",
      "Random Forest saved as 'random_forest_prediabetes_model.pkl'\n",
      "\n",
      "Logistic Regression (Hypertension) Accuracy: 1.0000\n",
      "[[94  0]\n",
      " [ 0 60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        94\n",
      "           1       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00       154\n",
      "   macro avg       1.00      1.00      1.00       154\n",
      "weighted avg       1.00      1.00      1.00       154\n",
      "\n",
      "Logistic Regression saved as 'logistic_regression_hypertension_model.pkl'\n",
      "\n",
      "Support Vector Machine (Hypertension) Accuracy: 0.9935\n",
      "[[93  1]\n",
      " [ 0 60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        94\n",
      "           1       0.98      1.00      0.99        60\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.99      0.99      0.99       154\n",
      "weighted avg       0.99      0.99      0.99       154\n",
      "\n",
      "Support Vector Machine saved as 'support_vector_machine_hypertension_model.pkl'\n",
      "\n",
      "Random Forest (Hypertension) Accuracy: 1.0000\n",
      "[[94  0]\n",
      " [ 0 60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        94\n",
      "           1       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00       154\n",
      "   macro avg       1.00      1.00      1.00       154\n",
      "weighted avg       1.00      1.00      1.00       154\n",
      "\n",
      "Random Forest saved as 'random_forest_hypertension_model.pkl'\n",
      "\n",
      "Please enter your details:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  aditya\n",
      "Enter Insulin level:  140\n",
      "Enter Glucose level:  130\n",
      "Enter Blood Pressure level:  140\n",
      "Enter Age:  40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aditya, Prediction from Logistic Regression: Prediabetes\n",
      "aditya, Prediction from Logistic Regression: Hypertension\n",
      "aditya, Prediction from Support Vector Machine: Prediabetes\n",
      "aditya, Prediction from Support Vector Machine: Hypertension\n",
      "aditya, Prediction from Random Forest: Prediabetes\n",
      "aditya, Prediction from Random Forest: Hypertension\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib  # Import joblib for saving the model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_diabetes_103.csv')\n",
    "\n",
    "# Print the column names to check what is available\n",
    "print(\"Columns in the dataset:\", data.columns)\n",
    "\n",
    "# Define the prediabetes criteria\n",
    "data['Prediabetes'] = ((data['Glucose'] >= 90) & (data['Glucose'] <= 125)) | \\\n",
    "                       (data['Insulin'] > 25) | \\\n",
    "                       (data['BloodPressure'] > 130)\n",
    "\n",
    "# Convert boolean to integer (1 for prediabetes, 0 for not)\n",
    "data['Prediabetes'] = data['Prediabetes'].astype(int)\n",
    "\n",
    "# Create the Hypertension feature based on the new risk conditions\n",
    "data['Hypertension'] = 0  # Initialize the column\n",
    "\n",
    "# Condition 1: Risk of 50% for hypertension if Insulin > 25 and Glucose > 100\n",
    "risk_condition_1 = (data['Insulin'] > 25) & (data['Glucose'] > 100)\n",
    "\n",
    "# Assign Hypertension based on the first risk condition\n",
    "data.loc[risk_condition_1 & (data['BloodPressure'] > 130), 'Hypertension'] = 1\n",
    "\n",
    "# Condition 2: Risk of 80% for hypertension if Glucose > 125\n",
    "risk_condition_2 = data['Glucose'] > 125\n",
    "\n",
    "# Assign Hypertension based on the second risk condition\n",
    "data.loc[risk_condition_2, 'Hypertension'] = 1\n",
    "\n",
    "# Fill Hypertension for those not meeting the conditions\n",
    "data['Hypertension'] = data['Hypertension'].fillna(0).astype(int)\n",
    "\n",
    "# Define features\n",
    "X = data[['Glucose', 'Insulin', 'BloodPressure', 'Age']]  # Include Age as a feature\n",
    "\n",
    "# Split the data for Prediabetes\n",
    "y_prediabetes = data['Prediabetes']  # Target variable for Prediabetes\n",
    "X_train, X_test, y_train_prediabetes, y_test_prediabetes = train_test_split(X, y_prediabetes, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data for Hypertension\n",
    "y_hypertension = data['Hypertension']  # Target variable for Hypertension\n",
    "X_train_hypertension, X_test_hypertension, y_train_hypertension, y_test_hypertension = train_test_split(X, y_hypertension, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train models for Prediabetes\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train_prediabetes)  # Train the model for Prediabetes\n",
    "    y_pred = model.predict(X_test_scaled)  # Make predictions\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test_prediabetes, y_pred)\n",
    "    print(f\"{model_name} (Prediabetes) Accuracy: {accuracy:.4f}\")\n",
    "    print(confusion_matrix(y_test_prediabetes, y_pred))\n",
    "    print(classification_report(y_test_prediabetes, y_pred))\n",
    "    \n",
    "    # Save the model to a file\n",
    "    joblib.dump(model, f'{model_name.replace(\" \", \"_\").lower()}_prediabetes_model.pkl')\n",
    "    print(f\"{model_name} saved as '{model_name.replace(' ', '_').lower()}_prediabetes_model.pkl'\\n\")\n",
    "\n",
    "# Train models for Hypertension\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train_hypertension)  # Train the model for Hypertension\n",
    "    y_pred = model.predict(X_test_scaled)  # Make predictions\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test_hypertension, y_pred)\n",
    "    print(f\"{model_name} (Hypertension) Accuracy: {accuracy:.4f}\")\n",
    "    print(confusion_matrix(y_test_hypertension, y_pred))\n",
    "    print(classification_report(y_test_hypertension, y_pred))\n",
    "    \n",
    "    # Save the model to a file\n",
    "    joblib.dump(model, f'{model_name.replace(\" \", \"_\").lower()}_hypertension_model.pkl')\n",
    "    print(f\"{model_name} saved as '{model_name.replace(' ', '_').lower()}_hypertension_model.pkl'\\n\")\n",
    "\n",
    "# Function to get user input\n",
    "def get_user_input():\n",
    "    name = input(\"Enter your name: \")\n",
    "    insulin = float(input(\"Enter Insulin level: \"))\n",
    "    glucose = float(input(\"Enter Glucose level: \"))\n",
    "    blood_pressure = float(input(\"Enter Blood Pressure level: \"))\n",
    "    age = int(input(\"Enter Age: \"))\n",
    "    \n",
    "    # Create a DataFrame for the input\n",
    "    new_data = pd.DataFrame({\n",
    "        'Glucose': [glucose],\n",
    "        'Insulin': [insulin],\n",
    "        'BloodPressure': [blood_pressure],\n",
    "        'Age': [age]\n",
    "    })\n",
    "    \n",
    "    return name, new_data\n",
    "\n",
    "# Function to make predictions\n",
    "def make_predictions(new_data):\n",
    "    # Load the scaler\n",
    "    scaler = joblib.load('scaler.pkl')  # Load the scaler\n",
    "\n",
    "    # Scale the new data\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "    # Make predictions on the new data using all models\n",
    "    predictions = {}\n",
    "    for model_name in models.keys():\n",
    "        # Predict for Prediabetes\n",
    "        model = joblib.load(f'{model_name.replace(\" \", \"_\").lower()}_prediabetes_model.pkl')  # Load the model\n",
    "        pred_prediabetes = model.predict(new_data_scaled)  # Make predictions\n",
    "        predictions[model_name] = {'Prediabetes': pred_prediabetes[0]}  # Store the prediction\n",
    "\n",
    "        # Predict for Hypertension\n",
    "        model = joblib.load(f'{model_name.replace(\" \", \"_\").lower()}_hypertension_model.pkl')  # Load the model\n",
    "        pred_hypertension = model.predict(new_data_scaled)  # Make predictions\n",
    "        predictions[model_name]['Hypertension'] = pred_hypertension[0]  # Store the prediction\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Main function to run the input and prediction process\n",
    "def main():\n",
    "    print(\"Please enter your details:\")\n",
    "    name, new_data = get_user_input()  # Get user input\n",
    "    predictions = make_predictions(new_data)  # Make predictions\n",
    "\n",
    "    # Output the predictions\n",
    "    for model_name, prediction in predictions.items():\n",
    "        if prediction['Prediabetes'] == 1:\n",
    "            print(f\"{name}, Prediction from {model_name}: Prediabetes\")\n",
    "        else:\n",
    "            print(f\"{name}, Prediction from {model_name}: No Prediabetes\")\n",
    "        \n",
    "        if prediction['Hypertension'] == 1:\n",
    "            print(f\"{name}, Prediction from {model_name}: Hypertension\")\n",
    "        else:\n",
    "            print(f\"{name}, Prediction from {model_name}: No Hypertension\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "417a976f-0ff0-4f7e-975a-d60a58846ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template_string, request\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the scaler and models\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "models = {\n",
    "    'Logistic Regression': joblib.load('logistic_regression_model.pkl'),\n",
    "    'Support Vector Machine': joblib.load('support_vector_machine_model.pkl'),\n",
    "    'Random Forest': joblib.load('random_forest_model.pkl')\n",
    "}\n",
    "\n",
    "# Function to make predictions\n",
    "def make_predictions(new_data):\n",
    "    # Scale the new data\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "    # Make predictions on the new data using all models\n",
    "    predictions = {}\n",
    "    for model_name, model in models.items():\n",
    "        # Predict for Prediabetes\n",
    "        pred_prediabetes = model.predict(new_data_scaled)  # Make predictions\n",
    "        predictions[model_name] = {'Prediabetes': pred_prediabetes[0]}  # Store the prediction\n",
    "\n",
    "        # Predict for Hypertension\n",
    "        model_hypertension = joblib.load(f'{model_name.replace(\" \", \"_\").lower()}_hypertension_model.pkl')  # Load the model\n",
    "        pred_hypertension = model_hypertension.predict(new_data_scaled)  # Make predictions\n",
    "        predictions[model_name]['Hypertension'] = pred_hypertension[0]  # Store the prediction\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# HTML templates\n",
    "index_html = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Diabetes Prediction</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Diabetes Prediction Form</h1>\n",
    "    <form method=\"POST\">\n",
    "        <label for=\"name\">Name:</label>\n",
    "        <input type=\"text\" id=\"name\" name=\"name\" required><br><br>\n",
    "        \n",
    "        <label for=\"insulin\">Insulin Level:</label>\n",
    "        <input type=\"number\" id=\"insulin\" name=\"insulin\" required><br><br>\n",
    "        \n",
    "        <label for=\"glucose\">Glucose Level:</label>\n",
    "        <input type=\"number\" id=\"glucose\" name=\"glucose\" required><br><br>\n",
    "        \n",
    "        <label for=\"blood_pressure\">Blood Pressure Level:</label>\n",
    "        <input type=\"number\" id=\"blood_pressure\" name=\"blood_pressure\" required><br><br>\n",
    "        \n",
    "        <label for=\"age\">Age:</label>\n",
    "        <input type=\"number\" id=\"age\" name=\"age\" required><br><br>\n",
    "        \n",
    "        <input type=\"submit\" value=\"Submit\">\n",
    "    </form>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "results_html = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Prediction Results</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Prediction Results for {{ name }}</h1>\n",
    "    {% for model_name, prediction in predictions.items() %}\n",
    "        <h2>{{ model_name }}</h2>\n",
    "        <p>Prediabetes: {{ 'Yes' if prediction['Prediabetes'] == 1 else 'No' }}</p>\n",
    "        <p>Hypertension: {{ 'Yes' if prediction['Hypertension'] == 1 else 'No' }}</p>\n",
    "    {% endfor %}\n",
    "    <a href=\"/\">Go Back</a>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        name = request.form['name']\n",
    "        insulin = float(request.form['insulin'])\n",
    "        glucose = float(request.form['glucose'])\n",
    "        blood_pressure = float(request.form['blood_pressure'])\n",
    "        age = int(request.form['age'])\n",
    "\n",
    "        # Create a DataFrame for the input\n",
    "        new_data = pd.DataFrame({\n",
    "            'Glucose': [glucose],\n",
    "            'Insulin': [insulin],\n",
    "            'BloodPressure': [blood_pressure],\n",
    "            'Age': [age]\n",
    "        })\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = make_predictions(new_data)\n",
    "\n",
    "        return render_template_string(results_html, name=name, predictions=predictions)\n",
    "\n",
    "    return render_template_string(index_html)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27a0e7aa-210d-446b-b3ef-84d10b64b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 13:14:11.299 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the scaler and models\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "models = {\n",
    "    'Logistic Regression': joblib.load('logistic_regression_model.pkl'),\n",
    "    'Support Vector Machine': joblib.load('support_vector_machine_model.pkl'),\n",
    "    'Random Forest': joblib.load('random_forest_model.pkl')\n",
    "}\n",
    "\n",
    "# Function to make predictions\n",
    "def make_predictions(new_data):\n",
    "    # Scale the new data\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "    # Make predictions on the new data using all models\n",
    "    predictions = {}\n",
    "    for model_name, model in models.items():\n",
    "        # Predict for Prediabetes\n",
    "        pred_prediabetes = model.predict(new_data_scaled)  # Make predictions\n",
    "        predictions[model_name] = {'Prediabetes': pred_prediabetes[0]}  # Store the prediction\n",
    "\n",
    "        # Predict for Hypertension\n",
    "        model_hypertension = joblib.load(f'{model_name.replace(\" \", \"_\").lower()}_hypertension_model.pkl')  # Load the model\n",
    "        pred_hypertension = model_hypertension.predict(new_data_scaled)  # Make predictions\n",
    "        predictions[model_name]['Hypertension'] = pred_hypertension[0]  # Store the prediction\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Streamlit app\n",
    "def main():\n",
    "    st.title(\"Diabetes Prediction App\")\n",
    "\n",
    "    # User input\n",
    "    name = st.text_input(\"Enter your name:\")\n",
    "    insulin = st.number_input(\"Insulin Level:\", min_value=0.0)\n",
    "    glucose = st.number_input(\"Glucose Level:\", min_value=0.0)\n",
    "    blood_pressure = st.number_input(\"Blood Pressure Level:\", min_value=0.0)\n",
    "    age = st.number_input(\"Age:\", min_value=0)\n",
    "\n",
    "    if st.button(\"Predict\"):\n",
    "        # Create a DataFrame for the input\n",
    "        new_data = pd.DataFrame({\n",
    "            'Glucose': [glucose],\n",
    "            'Insulin': [insulin],\n",
    "            'BloodPressure': [blood_pressure],\n",
    "            'Age': [age]\n",
    "        })\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = make_predictions(new_data)\n",
    "\n",
    "        # Display results\n",
    "        st.write(f\"### Prediction Results for {name}\")\n",
    "        for model_name, prediction in predictions.items():\n",
    "            st.write(f\"**{model_name}**\")\n",
    "            st.write(f\"Prediabetes: {'Yes' if prediction['Prediabetes'] == 1 else 'No'}\")\n",
    "            st.write(f\"Hypertension: {'Yes' if prediction['Hypertension'] == 1 else 'No'}\")\n",
    "            st.write(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75a50f-e2ed-44c2-a0d0-f2bc70c08794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
